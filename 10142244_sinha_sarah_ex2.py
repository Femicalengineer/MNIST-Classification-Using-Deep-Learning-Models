# -*- coding: utf-8 -*-
"""Copy of Copy of MMAI 894 Exercise 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k53ffwbj5Cmn23o95ZZdMF4W171R4xfW

# MMAI 894 - Exercise 2
## Convolutional artificial neural network : Image classification
The goal of this excercise is to build a convolutional neural network using the tensorflow/keras library. We will be using the MNIST dataset.
Submission instructions:

- You cannot edit this notebook directly. Save a copy to your drive, and make sure to identify yourself in the title using name and student number
- Do not insert new cells before the final one (titled "Further exploration") 
- Verify that your notebook can _restart and run all_. 
- Select File -> Download as .py (important! not as ipynb)
- Rename the file: `studentID_lastname_firstname_ex2.py`
- The mark will be assessed on the implementation of the functions with #TODO
- **Do not change anything outside the functions**  unless in the further exploration section
- The mark is not based on final accuracy - only on correctness
- Note: You do not have to answer the questions in thie notebook as part of your submission. They are meant to guide you.

- You should not need to use any additional libraries other than the ones listed below. You may want to import additional modules from those libraries, however.
"""

# Import modules
# Add modules as needed
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
import numpy as np

# For windows laptops add following 2 lines:
# import matplotlib
# matplotlib.use('agg')

import matplotlib.pyplot as plt

import tensorflow.keras as keras
from tensorflow.keras.utils import normalize,to_categorical

"""### Data preparation

#### Import data
"""

def load_data():
    # Import MNIST dataset from openml
    dataset = fetch_openml('mnist_784', version=1, data_home=None)

    # Data preparation
    raw_X = dataset['data']
    raw_Y = dataset['target']
    return raw_X, raw_Y

raw_X, raw_Y = load_data()

"""## Consider the following
- Same as excercise 1
- what shape should x be for a convolutional network?
"""

def clean_data(raw_X, raw_Y):
    # TODO: clean and QA raw_X and raw_Y
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    X= raw_X.to_numpy()
    X = X/255
    cleaned_X=np.zeros((len(raw_X),28,28,1))

    for i in range(len(X)):
      cleaned_X[i]=np.reshape(X[i],(28,28,1))

    Y=raw_Y.to_numpy()
    cleaned_Y=to_categorical(Y)

    return cleaned_X, cleaned_Y

cleaned_X, cleaned_Y = clean_data(raw_X, raw_Y)

"""#### Data split

- Split your data into a train set (50%), validation set (20%) and a test set (30%). You can use scikit-learn's train_test_split function.
"""

def split_data(cleaned_X, cleaned_Y):
    # TODO: split the data
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    X_train, X_other, Y_train, y_other = train_test_split(cleaned_X, cleaned_Y, test_size=0.5, random_state=42)
    X_val, X_test, Y_val, Y_test = train_test_split(X_other, y_other, test_size=0.6, random_state=42)
    
    return X_val, X_test, X_train, Y_val, Y_test, Y_train

X_val, X_test, X_train, Y_val, Y_test, Y_train = split_data(cleaned_X, cleaned_Y)

"""### Model

#### Neural network structure

This time, the exact model architecture is left to you to explore.  
Keep the number of parameters below 2,000,000
"""

def build_model():
    # TODO: build the model, 
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    # set input shape is 28x28x1 because images are 28x28 pixels, with a single color channel 
    input_shape = (28,28,1)

    model = keras.Sequential(
        [
         keras.Input(shape=input_shape),
         keras.layers.Conv2D(32,kernel_size=(3,3), activation='relu'),
         keras.layers.MaxPooling2D(pool_size=(2, 2)),
         keras.layers.Conv2D(128,kernel_size=(3,3), activation='relu',input_shape=input_shape),
         keras.layers.MaxPooling2D(pool_size=(2, 2)),
         keras.layers.Conv2D(256,kernel_size=(3,3), activation='relu',input_shape=input_shape),
         keras.layers.MaxPooling2D(pool_size=(2, 2)),
         keras.layers.Flatten(),
         keras.layers.Dense(128,activation='relu'),
         keras.layers.Dropout(0.5),
         keras.layers.Dense(10,activation='softmax')
        ]
    )
    
    
    return model

def compile_model(model):
    # TODO: compile the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

    return model

def train_model(model, X_train, Y_train, X_val, Y_val):
    # TODO: train the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION
    batch_size=128
    epochs=15


    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val,Y_val))
    return model, history


def eval_model(model, X_test, Y_test):
    # TODO: evaluate the model
    # DO NOT CHANGE THE INPUTS OR OUTPUTS TO THIS FUNCTION

    test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=0)


    return test_loss, test_accuracy

## You may use this space (and add additional cells for exploration)

model = build_model()
model = compile_model(model)
model, history = train_model(model, X_train, Y_train, X_val, Y_val)
test_loss, test_accuracy = eval_model(model, X_test, Y_test)




"""### Create a model with hyperparameter tuning using Optuna"""

# pip install optuna

import optuna

def objective(trial):

  input_shape = (28,28,1)
  batch_size=128
  epochs=15


  # Import MNIST dataset from openml
  dataset = fetch_openml('mnist_784', version=1, data_home=None)

  # Data preparation
  raw_X = dataset['data']
  raw_Y = dataset['target']

  X= raw_X.to_numpy()
  X = X/255
  cleaned_X=np.zeros((len(raw_X),28,28,1))
  for i in range(len(X)):
    cleaned_X[i]=np.reshape(X[i],(28,28,1))
  Y=raw_Y.to_numpy()
  cleaned_Y=to_categorical(Y)


  #split the data
  X_train, X_other, Y_train, y_other = train_test_split(cleaned_X, cleaned_Y, test_size=0.5, random_state=42)
  X_val, X_test, Y_val, Y_test = train_test_split(X_other, y_other, test_size=0.6, random_state=42)


  model = keras.Sequential(
      [
        keras.layers.Conv2D(
            filters=trial.suggest_categorical("filters1", [32, 64]),
            kernel_size=trial.suggest_categorical("kernel_size1", [3, 5]),
            strides=trial.suggest_categorical("strides", [1, 2]),
            activation='relu',
            input_shape=input_shape),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Conv2D(
            filters=trial.suggest_categorical("filters2", [64,128]),
            kernel_size=trial.suggest_categorical("kernel_size2", [3, 5]),
            activation='relu'),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Flatten(),
        keras.layers.Dense(128,activation='relu'),
        keras.layers.Dropout(0.5),
        keras.layers.Dense(10,activation='softmax')
        ]
    )
  

  # We compile our model with a sampled learning rate.
  lr = trial.suggest_categorical("lr", [1e-3, 1e-2])
  model.compile(
  loss="categorical_crossentropy", optimizer=keras.optimizers.RMSprop(learning_rate=lr), metrics=["accuracy"])


  model.fit(
    X_train,
    Y_train,
    validation_data=(X_val, Y_val),
    shuffle=True,
    batch_size=batch_size,
    epochs=epochs,
    verbose=False,
    )
  

  score = model.evaluate(X_test, Y_test, verbose=0)

  return score[1]


study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30, timeout=2000, show_progress_bar=True)
print("Number of finished trials: {}".format(len(study.trials)))
print("Best trial:")
trial = study.best_trial
print("  Test Accuracy of Tuned Model: {}".format(trial.value))
print("  Params of optimal CNN model: ")
for key, value in trial.params.items():
  print("    {}: {}".format(key, value))





